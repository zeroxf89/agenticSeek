<p align="center">
<img align="center" src="./media/whale_readme.jpg">
<p>

--------------------------------------------------------------------------------
[English](./README.md) | [ç¹é«”ä¸­æ–‡](./README_CHT.md) | FranÃ§ais

# AgenticSeek: Une IA comme Manus mais Ã  base d'agents DeepSeek R1 fonctionnant en local.

Une alternative **entiÃ¨rement locale** Ã  Manus AI, un assistant vocal IA qui code, explore votre systÃ¨me de fichiers, navigue sur le web et corrige ses erreurs, tout cela sans envoyer la moindre donnÃ©e dans le cloud. Construit avec des modÃ¨les de raisonnement comme DeepSeek R1, cet agent autonome fonctionne entiÃ¨rement sur votre hardware, garantissant la confidentialitÃ© de vos donnÃ©es.

[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/4Ub2D6Fj)

> ğŸ› ï¸ **En cours de dÃ©veloppement** â€“ On cherche activement des contributeurs!

![alt text](./media/whale_readme.jpg)

> *Recherche sur le web des activitÃ©s Ã  faire Ã  Paris*

> *Code le jeu snake en python*

> *J'aimerais que tu trouve une api mÃ©tÃ©o et que tu me code un application qui affiche la mÃ©tÃ©o Ã  Toulouse*


### agenticSeek peut planifier des taches!

![alt text](./media/exemples/demo_image.png)

## FonctionnalitÃ©s:

- **100% Local**: Fonctionne en local sur votre PC. Vos donnÃ©es restent les vÃ´tres. 

- **AccÃ¨s Ã  vos Fichiers**: Utilise bash pour naviguer et manipuler vos fichiers.

- **Codage semi-autonome**: Peut Ã©crire, dÃ©boguer et exÃ©cuter du code en Python, C, Golang et d'autres langages Ã  venir. 

- **Routage d'Agent**: SÃ©lectionne automatiquement lâ€™agent appropriÃ© pour la tÃ¢che. 

- **Planification**: Pour les taches complexe utilise plusieurs agents.

- **Navigation Web Autonome**: Navigation web autonome.

- **Memoire efficace**: Gestion efficace de la mÃ©moire et des sessions. 

---

## **Installation**

Assurez-vous dâ€™avoir installÃ© le pilote Chrome, Docker et Python 3.10 (ou une version plus rÃ©cente).

Pour les problÃ¨mes liÃ©s au pilote Chrome, consultez la section Chromedriver.

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t et configurer

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2 **CrÃ©er un environnement virtuel**

```sh
python3 -m venv agentic_seek_env
source agentic_seek_env/bin/activate     
# On Windows: agentic_seek_env\Scripts\activate
```

### 3ï¸âƒ£ **Installation**

**Automatique:**

```sh
./install.sh
```

**Manuel:**

```sh
pip3 install -r requirements.txt
```


## Faire fonctionner sur votre machine 

**Nous recommandons dâ€™utiliser au moins DeepSeek 14B, les modÃ¨les plus petits ont du mal avec lâ€™utilisation des outils et oublient rapidement le contexte.**

### 1ï¸âƒ£ **TÃ©lÃ©chargement du modÃ¨le**  

Assurer vous d'avoir [Ollama](https://ollama.com/) installÃ©.

TÃ©lÃ©charger `deepseek-r1:14b` de [DeepSeek](https://deepseek.com/models)

```sh
ollama pull deepseek-r1:14b
```

### 2ï¸ **DÃ©marrage d'ollama**  

```sh
ollama serve
```

Modifiez le fichier config.ini pour dÃ©finir provider_name sur ollama et provider_model sur deepseek-r1:14b

```sh
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:14b
provider_server_address = 127.0.0.1:11434
```

dÃ©marrer tous les services :

```sh
sudo ./start_services.sh
```

Lancer l'assitant:

```sh
python3 main.py
```

Voir la section **Utilisation** si vous ne comprenez pas comment lâ€™utiliser

Voir la section **ProblÃ¨mes** connus si vous rencontrez des problÃ¨mes

Voir la section **ExÃ©cuter** avec une API si votre matÃ©riel ne peut pas exÃ©cuter DeepSeek localement

Voir la section **Configuration** pour une explication dÃ©taillÃ©e du fichier de configuration.

---

## Utilisation

Assurez-vous que les services sont en cours dâ€™exÃ©cution avec ./start_services.sh et lancez AgenticSeek avec python3 main.py

```sh
sudo ./start_services.sh
python3 main.py
```

Vous verrez un prompt: ">>> "
Cela indique quâ€™AgenticSeek attend que vous saisissiez des instructions.
Vous pouvez Ã©galement utiliser la reconnaissance vocale en dÃ©finissant listen = True dans la configuration.

Pour quitter, dites simplement `goodbye`.

Voici quelques exemples dâ€™utilisation :

### Programmation

> *Aide-moi avec la multiplication de matrices en Golang*

> *Initalize un nouveau project python, setup le readme, gitignore et tout le bordel et fait un premier commit*

> *Fais un jeu snake en Python*

### Recherche web

> *Fais une recherche sur le web pour trouver des startups technologiques au Japon qui travaillent sur des recherches avancÃ©es en IA*

> *Peux-tu trouver sur internet qui a crÃ©Ã© agenticSeek ?*

> *Peux-tu trouver sur quel site je peux acheter une RTX 4090 Ã  bas prix ?*

### Fichier

> *HÃ©, peux-tu trouver oÃ¹ est million_dollars_contract.pdf ? Je lâ€™ai perdu*

> *Montre-moi combien dâ€™espace il me reste sur mon disque*

> *Trouve et lis le fichier README.md et suis les instructions dâ€™installation*

### Conversation

> *Parle-moi de la France*

> *Quel est le sens de la vie ?*

> *Donne moi une recette avec les ingrÃ©dients suivant de mon frigo...*

AprÃ¨s avoir saisi votre requÃªte, AgenticSeek attribuera le meilleur agent pour la tÃ¢che.

Le systÃ¨me de routage des agents peut parfois ne pas toujours attribuer le bon agent en fonction de votre requÃªte.

Par consÃ©quent, vous devez Ãªtre assez explicite sur ce que vous voulez et sur la maniÃ¨re dont lâ€™IA doit procÃ©der. Par exemple, si vous voulez quâ€™elle effectue une recherche sur le web, ne dites pas :

Connait-tu de bons pays pour voyager seul ?

Dites plutÃ´t :

Fait une recherche sur le web, quels sont les meilleurs pays pour voyager seul?

---

## **ExÃ©cuter le LLM sur votre propre serveur**  

Si vous disposez dâ€™un ordinateur puissant ou dâ€™un serveur que vous voulez utiliser, mais que vous souhaitez y accÃ©der depuis votre ordinateur portable, vous avez la possibilitÃ© dâ€™exÃ©cuter le LLM sur un serveur distant.

### 1ï¸âƒ£  **Configurer et dÃ©marrer les scripts du serveur** 

Sur votre "serveur" qui exÃ©cutera le modÃ¨le IA, obtenez lâ€™adresse IP

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1
```

Remarque : Pour Windows ou macOS, utilisez respectivement ipconfig ou ifconfig pour trouver lâ€™adresse IP.

**Si vous souhaitez utiliser un fournisseur basÃ© sur OpenAI, suivez la section ExÃ©cuter avec une API.**

Clonez le dÃ©pÃ´t et entrez dans le dossier server/.


```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/server/
```

Installez les dÃ©pendances spÃ©cifiques au serveur :

```sh
pip3 install -r requirements.txt
```

ExÃ©cutez le script du serveur.

```sh
python3 app.py --provider ollama --port 3333
```

Vous avez le choix entre utiliser ollama et llamacpp comme service LLM.

### 2ï¸âƒ£ **Lancer** 

Maintenant, sur votre ordinateur personnel :

Modifiez le fichier config.ini pour dÃ©finir provider_name sur server et provider_model sur deepseek-r1:14b.

DÃ©finissez provider_server_address sur lâ€™adresse IP de la machine qui exÃ©cutera le modÃ¨le.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:14b
provider_server_address = x.x.x.x:3333
```

ExÃ©cutez lâ€™assistant :

```sh
sudo ./start_services.sh
python3 main.py
```

## **ExÃ©cuter avec une API**  

AVERTISSEMENT : Assurez-vous quâ€™il nâ€™y a pas dâ€™espace en fin de ligne dans la configuration.

DÃ©finissez is_local sur True si vous utilisez une API basÃ©e sur OpenAI localement.

Changez lâ€™adresse IP si votre API basÃ©e sur OpenAI fonctionne sur votre propre serveur.

```sh
[MAIN]
is_local = False
provider_name = openai
provider_model = gpt-4o
provider_server_address = 127.0.0.1:5000
```

ExÃ©cutez lâ€™assistant :

```sh
sudo ./start_services.sh
python3 main.py
```

## Config

Exemple de configuration :
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:1.5b
provider_server_address = 127.0.0.1:11434
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False
work_dir =  /Users/mlg/Documents/ai_folder
jarvis_personality = False
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explanation**:

`is_local` -> ExÃ©cute lâ€™agent localement (True) ou sur un serveur distant (False).

`provider_name` -> Le fournisseur Ã  utiliser (parmi : ollama, server, lm-studio, deepseek-api).

`provider_model` -> Le modÃ¨le utilisÃ©, par exemple, deepseek-r1:1.5b.

`provider_server_address` -> Adresse du serveur, par exemple, 127.0.0.1:11434 pour local. DÃ©finissez nâ€™importe quoi pour une API non locale.

`agent_name` -> Nom de lâ€™agent, par exemple, Friday. UtilisÃ© comme mot dÃ©clencheur pour la reconnaissance vocale.

`recover_last_session` -> Reprend la derniÃ¨re session (True) ou non (False).

`save_session` -> Sauvegarde les donnÃ©es de la session (True) ou non (False).

`speak` -> Active la sortie vocale (True) ou non (False).

`listen` -> Ã‰coute les entrÃ©es vocales (True) ou non (False).

`work_dir` -> Dossier auquel lâ€™IA aura accÃ¨s, par exemple : /Users/user/Documents/.

`jarvis_personality` -> Utilise une personnalitÃ© de type JARVIS (True) ou non (False). Cela modifie simplement le fichier de prompt.

`headless_browser` -> ExÃ©cute le navigateur sans fenÃªtre visible (True) ou non (False).

`stealth_mode` -> Rend la dÃ©tection des bots plus difficile. Le seul inconvÃ©nient est que vous devez installer manuellement lâ€™extension anticaptcha.



## Providers

Le tableau ci-dessous montre les LLM providers disponibles :

| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Yes    | ExÃ©cutez des LLM localement avec facilitÃ© en utilisant Ollama comme fournisseur LLM 
| server    | Yes    | HÃ©bergez le modÃ¨le sur une autre machine, exÃ©cutez sur votre machine locale 
| lm-studio  | Yes    | ExÃ©cutez un LLM localement avec LM Studio (dÃ©finissez provider_name sur lm-studio) 
| openai    | No     | Utilise ChatGPT API (pas privÃ©) |
| deepseek-api  | No     | Deepseek API (pas privÃ©) |
| huggingface| No    | Hugging-Face API (pas privÃ©) |

Pour sÃ©lectionner un provider LLM, modifiez le config.ini :

```
is_local = False
provider_name = openai
provider_model = gpt-4o
provider_server_address = 127.0.0.1:5000
```

`is_local` : doit Ãªtre True pour tout LLM exÃ©cutÃ© localement, sinon False.

`provider_name` : SÃ©lectionnez le fournisseur Ã  utiliser par son nom, voir la liste des fournisseurs ci-dessus.

`provider_model` : DÃ©finissez le modÃ¨le Ã  utiliser par lâ€™agent.

`provider_server_address` : peut Ãªtre dÃ©fini sur nâ€™importe quoi si vous nâ€™utilisez pas le fournisseur server.

# ProblÃ¨mes connus 

## ProblÃ¨mes avec Chromedriver

Erreur #1:**incompatibilitÃ©**

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Cela se produit sâ€™il y a une incompatibilitÃ© entre votre navigateur et la version de chromedriver.

Vous devez naviguer pour tÃ©lÃ©charger la derniÃ¨re version :

https://developer.chrome.com/docs/chromedriver/downloads

Si vous utilisez Chrome version 115 ou plus rÃ©cent, allez sur :

https://googlechromelabs.github.io/chrome-for-testing/

Et tÃ©lÃ©chargez la version de chromedriver correspondant Ã  votre systÃ¨me dâ€™exploitation.

![alt text](./media/chromedriver_readme.png)

Si cette section est incomplÃ¨te, merci de faire une nouvelle issue github.

## FAQ

**Q: J'ai besoin d'un gros PC?**  

Ã§a dÃ©pend du modÃ¨le!
Pour un modÃ¨le 7B : GPU avec 8 Go de VRAM.
Pour un modÃ¨le 14B : GPU 12 Go (par exemple, RTX 3060).
Et un modÃ¨le 32B : 24 Go+ de VRAM.

**Q: Pourquoi deepseek et pas un autre modÃ¨le**  

DeepSeek R1 excelle dans le raisonnement et lâ€™utilisation dâ€™outils pour sa taille. Nous pensons que câ€™est un choix solide pour nos besoins, bien que dâ€™autres modÃ¨les fonctionnent Ã©galement (bien que moins bien pour un nombre identique de paramÃ¨tre).

**Q: J'ai une erreur quand je lance le programme, je fait quoi?**  

Assurez-vous quâ€™Ollama est en cours dâ€™exÃ©cution (ollama serve), que votre config.ini correspond Ã  votre fournisseur, et que les dÃ©pendances sont installÃ©es. Si cela ne fonctionne pas, nâ€™hÃ©sitez pas Ã  signaler un problÃ¨me.

**Q: C'est vraiment 100% local?**  

Oui, avec les fournisseurs Ollama, lm-studio ou Server, toute la reconnaissance vocale, le LLM et la synthÃ¨se vocale fonctionnent localement. Les options non locales (OpenAI ou autres API) sont facultatives.

**Q: En quoi c'est supÃ©rieur Ã  Manus**

Il ne l'est pas, mais nous privilÃ©gions lâ€™exÃ©cution locale et la confidentialitÃ© par rapport Ã  une approche basÃ©e sur le cloud. Câ€™est une alternative plus accessible et surtout moins cher !

## Contribute

Nous recherchons des dÃ©veloppeurs pour amÃ©liorer AgenticSeek ! Consultez la section "issues" github ou les discussions.

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)

## Auteurs:
 > [Fosowl](https://github.com/Fosowl) - Epitech 2024, France
 > [steveh8758](https://github.com/steveh8758) - UniversitÃ© Feng Chia, Taiwan
