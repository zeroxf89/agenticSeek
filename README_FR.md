


# AgenticSeek: Une IA comme Manus mais Ã  base d'agents DeepSeek R1 fonctionnant en local.

Une alternative **entiÃ¨rement locale** Ã  Manus AI, un assistant vocal IA qui code, explore votre systÃ¨me de fichiers, navigue sur le web et corrige ses erreurs, tout cela sans envoyer la moindre donnÃ©e dans le cloud. Construit avec des modÃ¨les de raisonnement comme DeepSeek R1, cet agent autonome fonctionne entiÃ¨rement sur votre hardware, garantissant la confidentialitÃ© de vos donnÃ©es.

[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/4Ub2D6Fj)

> ğŸ› ï¸ **En cours de dÃ©veloppement** â€“ On cherche activement des contributeurs!

![alt text](./media/whale_readme.jpg)

> *Do a deep search of AI startup in Osaka and Tokyo, find at least 5, then save in the research_japan.txt file*

> *Can you make a tetris game in C ?*

> *I would like to setup a new project file index as mark2.*


### agenticSeek peut planifier des taches!

![alt text](./media/exemples/demo_image.png)

## FonctionnalitÃ©s:

- **100% Local**: Fonctionne en local sur votre PC. Vos donnÃ©es restent les vÃ´tres. 

- **AccÃ¨s Ã  vos Fichiers**: Utilise bash pour naviguer et manipuler vos fichiers.

- **Codage semi-autonome**: Peut Ã©crire, dÃ©boguer et exÃ©cuter du code en Python, C, Golang et d'autres langages Ã  venir. 

- **Routage d'Agent**: SÃ©lectionne automatiquement lâ€™agent appropriÃ© pour la tÃ¢che. 

- **Planification**: Pour les taches complexe utilise plusieurs agents.

- **Navigation Web Autonome**: Navigation web autonome.

- **Memoire efficace**: Gestion efficace de la mÃ©moire et des sessions. 

---

## **Installation**

Assurez-vous dâ€™avoir installÃ© le pilote Chrome, Docker et Python 3.10 (ou une version plus rÃ©cente).

Pour les problÃ¨mes liÃ©s au pilote Chrome, consultez la section Chromedriver.

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t et configurer

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2 **CrÃ©er un environnement virtuel**

```sh
python3 -m venv agentic_seek_env
source agentic_seek_env/bin/activate     
# On Windows: agentic_seek_env\Scripts\activate
```

### 3ï¸âƒ£ **Installation**

**Automatique:**

```sh
./install.sh
```

**Manuel:**

```sh
pip3 install -r requirements.txt
```


## Faire fonctionner sur votre machine 

**Nous recommandons dâ€™utiliser au moins DeepSeek 14B, les modÃ¨les plus petits ont du mal avec lâ€™utilisation des outils et oublient rapidement le contexte.**

### 1ï¸âƒ£ **TÃ©lÃ©chargement du modÃ¨le**  

Assurer vous d'avoir [Ollama](https://ollama.com/) installÃ©.

TÃ©lÃ©charger `deepseek-r1:14b` de [DeepSeek](https://deepseek.com/models)

```sh
ollama pull deepseek-r1:14b
```

### 2ï¸ **DÃ©marrage d'ollama**  

```sh
ollama serve
```

Modifiez le fichier config.ini pour dÃ©finir provider_name sur ollama et provider_model sur deepseek-r1:14b

```sh
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:14b
provider_server_address = 127.0.0.1:11434
```

dÃ©marrer tous les services :

```sh
sudo ./start_services.sh
```

Lancer l'assitant:

```sh
python3 main.py
```

Voir la section **Utilisation** si vous ne comprenez pas comment lâ€™utiliser

Voir la section **ProblÃ¨mes** connus si vous rencontrez des problÃ¨mes

Voir la section **ExÃ©cuter** avec une API si votre matÃ©riel ne peut pas exÃ©cuter DeepSeek localement

Voir la section **Configuration** pour une explication dÃ©taillÃ©e du fichier de configuration.

---

## Utilisation

Avertissement : actuellement, le systÃ¨me qui choisit le meilleur agent IA fonctionnera mal avec du texte non anglophone. Cela est dÃ» au fait que le routage des agents utilise un modÃ¨le entraÃ®nÃ© sur du texte en anglais. Nous travaillons dur pour corriger cela. Veuillez utiliser lâ€™anglais pour le moment.

Assurez-vous que les services sont en cours dâ€™exÃ©cution avec ./start_services.sh et lancez AgenticSeek avec python3 main.py

```sh
sudo ./start_services.sh
python3 main.py
```

Vous verrez un prompt: ">>> "
Cela indique quâ€™AgenticSeek attend que vous saisissiez des instructions.
Vous pouvez Ã©galement utiliser la reconnaissance vocale en dÃ©finissant listen = True dans la configuration.

Pour quitter, dites simplement `goodbye`.

Voici quelques exemples dâ€™utilisation :

### Programmation

> *Help me with matrix multiplication in Golang*

> *Scan my network with nmap, find if any suspicious devices is connected*

> *Make a snake game in python*

### Recherche web

> *Do a web search to find cool tech startup in Japan working on cutting edge AI research*

> *Can you find on the internet who created agenticSeek?*

> *Can you find on which website I can buy a rtx 4090 for cheap*

### Fichier

> *Hey can you find where is million_dollars_contract.pdf i lost it*

> *Show me how much space I have left on my disk*

> *Find and read the README.md and follow the install instruction*

### Conversation

> *Tell me about France*

> *What is the meaning of life ?*

> *Should I take creatine before or after workout?*


AprÃ¨s avoir saisi votre requÃªte, AgenticSeek attribuera le meilleur agent pour la tÃ¢che.

Comme il sâ€™agit dâ€™un prototype, le systÃ¨me de routage des agents pourrait ne pas toujours attribuer le bon agent en fonction de votre requÃªte.

Par consÃ©quent, vous devez Ãªtre explicite sur ce que vous voulez et sur la maniÃ¨re dont lâ€™IA doit procÃ©der. Par exemple, si vous voulez quâ€™elle effectue une recherche sur le web, ne dites pas :

Connait-tu de bons pays pour voyager seul ?

Dites plutÃ´t :

Fait une recherche sur le web, quels sont les meilleurs pays pour voyager seul?

---

## **ExÃ©cuter le LLM sur votre propre serveur**  

Si vous disposez dâ€™un ordinateur puissant ou dâ€™un serveur que vous voulez utiliser, mais que vous souhaitez y accÃ©der depuis votre ordinateur portable, vous avez la possibilitÃ© dâ€™exÃ©cuter le LLM sur un serveur distant.

### 1ï¸âƒ£  **Configurer et dÃ©marrer les scripts du serveur** 

Sur votre "serveur" qui exÃ©cutera le modÃ¨le IA, obtenez lâ€™adresse IP

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1
```

Remarque : Pour Windows ou macOS, utilisez respectivement ipconfig ou ifconfig pour trouver lâ€™adresse IP.

**Si vous souhaitez utiliser un fournisseur basÃ© sur OpenAI, suivez la section ExÃ©cuter avec une API.**

Clonez le dÃ©pÃ´t et entrez dans le dossier server/.


```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/server/
```

Installez les dÃ©pendances spÃ©cifiques au serveur :

```sh
pip3 install -r requirements.txt
```

ExÃ©cutez le script du serveur.

```sh
python3 app.py --provider ollama --port 3333
```

Vous avez le choix entre utiliser ollama et llamacpp comme service LLM.

### 2ï¸âƒ£ **Lancer** 

Maintenant, sur votre ordinateur personnel :

Modifiez le fichier config.ini pour dÃ©finir provider_name sur server et provider_model sur deepseek-r1:14b.

DÃ©finissez provider_server_address sur lâ€™adresse IP de la machine qui exÃ©cutera le modÃ¨le.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:14b
provider_server_address = x.x.x.x:3333
```

ExÃ©cutez lâ€™assistant :

```sh
sudo ./start_services.sh
python3 main.py
```

## **ExÃ©cuter avec une API**  

AVERTISSEMENT : Assurez-vous quâ€™il nâ€™y a pas dâ€™espace en fin de ligne dans la configuration.

DÃ©finissez is_local sur True si vous utilisez une API basÃ©e sur OpenAI localement.

Changez lâ€™adresse IP si votre API basÃ©e sur OpenAI fonctionne sur votre propre serveur.

```sh
[MAIN]
is_local = False
provider_name = openai
provider_model = gpt-4o
provider_server_address = 127.0.0.1:5000
```

ExÃ©cutez lâ€™assistant :

```sh
sudo ./start_services.sh
python3 main.py
```

## Config

Exemple de configuration :
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:1.5b
provider_server_address = 127.0.0.1:11434
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False
work_dir =  /Users/mlg/Documents/ai_folder
jarvis_personality = False
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explanation**:

`is_local` -> ExÃ©cute lâ€™agent localement (True) ou sur un serveur distant (False).

`provider_name` -> Le fournisseur Ã  utiliser (parmi : ollama, server, lm-studio, deepseek-api).

`provider_model` -> Le modÃ¨le utilisÃ©, par exemple, deepseek-r1:1.5b.

`provider_server_address` -> Adresse du serveur, par exemple, 127.0.0.1:11434 pour local. DÃ©finissez nâ€™importe quoi pour une API non locale.

`agent_name` -> Nom de lâ€™agent, par exemple, Friday. UtilisÃ© comme mot dÃ©clencheur pour la reconnaissance vocale.

`recover_last_session` -> Reprend la derniÃ¨re session (True) ou non (False).

`save_session` -> Sauvegarde les donnÃ©es de la session (True) ou non (False).

`speak` -> Active la sortie vocale (True) ou non (False).

`listen` -> Ã‰coute les entrÃ©es vocales (True) ou non (False).

`work_dir` -> Dossier auquel lâ€™IA aura accÃ¨s, par exemple : /Users/user/Documents/.

`jarvis_personality` -> Utilise une personnalitÃ© de type JARVIS (True) ou non (False). Cela modifie simplement le fichier de prompt.

`headless_browser` -> ExÃ©cute le navigateur sans fenÃªtre visible (True) ou non (False).

`stealth_mode` -> Rend la dÃ©tection des bots plus difficile. Le seul inconvÃ©nient est que vous devez installer manuellement lâ€™extension anticaptcha.



## Providers

Le tableau ci-dessous montre les fournisseurs disponibles :

| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Yes    | ExÃ©cutez des LLM localement avec facilitÃ© en utilisant Ollama comme fournisseur LLM 
| server    | Yes    | HÃ©bergez le modÃ¨le sur une autre machine, exÃ©cutez sur votre machine locale 
| lm-studio  | Yes    | ExÃ©cutez un LLM localement avec LM Studio (dÃ©finissez provider_name sur lm-studio) 
| openai    | No     | Utilise ChatGPT API (pas privÃ©) |
| deepseek-api  | No     | Deepseek API (pas privÃ©) |
| huggingface| No    | Hugging-Face API (pas privÃ©) |

Pour sÃ©lectionner un fournisseur, modifiez le config.ini :

```
is_local = False
provider_name = openai
provider_model = gpt-4o
provider_server_address = 127.0.0.1:5000
```

`is_local` : doit Ãªtre True pour tout LLM exÃ©cutÃ© localement, sinon False.

`provider_name` : SÃ©lectionnez le fournisseur Ã  utiliser par son nom, voir la liste des fournisseurs ci-dessus.

`provider_model` : DÃ©finissez le modÃ¨le Ã  utiliser par lâ€™agent.

`provider_server_address` : peut Ãªtre dÃ©fini sur nâ€™importe quoi si vous nâ€™utilisez pas le fournisseur server.

# ProblÃ¨mes connus 

## ProblÃ¨mes avec Chromedriver

Erreur #1:**incompatibilitÃ©**

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Cela se produit sâ€™il y a une incompatibilitÃ© entre votre navigateur et la version de chromedriver.

Vous devez naviguer pour tÃ©lÃ©charger la derniÃ¨re version :

https://developer.chrome.com/docs/chromedriver/downloads

Si vous utilisez Chrome version 115 ou plus rÃ©cent, allez sur :

https://googlechromelabs.github.io/chrome-for-testing/

Et tÃ©lÃ©chargez la version de chromedriver correspondant Ã  votre systÃ¨me dâ€™exploitation.

![alt text](./media/chromedriver_readme.png)

Si cette section est incomplÃ¨te, veuillez signaler un problÃ¨me.

## FAQ

**Q: What hardware do I need?**  

ModÃ¨le 7B : GPU avec 8 Go de VRAM.
ModÃ¨le 14B : GPU 12 Go (par exemple, RTX 3060).
ModÃ¨le 32B : 24 Go+ de VRAM.

**Q: Why Deepseek R1 over other models?**  

DeepSeek R1 excelle dans le raisonnement et lâ€™utilisation dâ€™outils pour sa taille. Nous pensons que câ€™est un choix solide pour nos besoins, bien que dâ€™autres modÃ¨les fonctionnent Ã©galement bien, DeepSeek est notre choix principal.

**Q: I get an error running `main.py`. What do I do?**  

Assurez-vous quâ€™Ollama est en cours dâ€™exÃ©cution (ollama serve), que votre config.ini correspond Ã  votre fournisseur, et que les dÃ©pendances sont installÃ©es. Si cela ne fonctionne pas, nâ€™hÃ©sitez pas Ã  signaler un problÃ¨me.

**Q: Can it really run 100% locally?**  

Oui, avec les fournisseurs Ollama ou Server, toute la reconnaissance vocale, le LLM et la synthÃ¨se vocale fonctionnent localement. Les options non locales (OpenAI ou autres API) sont facultatives.

**Q: How come it is older than manus ?**

Nous avons commencÃ© cela comme un projet amusant pour crÃ©er une IA locale de type Jarvis. Cependant, avec lâ€™Ã©mergence de Manus, nous avons vu lâ€™opportunitÃ© de rÃ©orienter certaines tÃ¢ches pour en faire une autre alternative.

**Q: How is it better than manus  ?**

Il ne lâ€™est pas, mais nous privilÃ©gions lâ€™exÃ©cution locale et la confidentialitÃ© par rapport Ã  une approche basÃ©e sur le cloud. Câ€™est une alternative amusante et accessible !

## Contribute

Nous recherchons des dÃ©veloppeurs pour amÃ©liorer AgenticSeek ! Consultez les problÃ¨mes ouverts ou les discussions.

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)

## Auteurs:
 > [Fosowl](https://github.com/Fosowl)
 > [steveh8758](https://github.com/steveh8758) 
